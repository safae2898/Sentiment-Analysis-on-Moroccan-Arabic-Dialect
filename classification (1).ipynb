{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "333c3923",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-01-21T22:28:14.505440Z",
     "iopub.status.busy": "2022-01-21T22:28:14.504719Z",
     "iopub.status.idle": "2022-01-21T22:28:14.510229Z",
     "shell.execute_reply": "2022-01-21T22:28:14.510716Z",
     "shell.execute_reply.started": "2022-01-21T22:25:09.506495Z"
    },
    "papermill": {
     "duration": 0.042507,
     "end_time": "2022-01-21T22:28:14.510994",
     "exception": false,
     "start_time": "2022-01-21T22:28:14.468487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/sentiment-analysis-on-moroccan-arabic-dialect/sample_submission.csv\n",
      "/kaggle/input/sentiment-analysis-on-moroccan-arabic-dialect/test_stage1.csv\n",
      "/kaggle/input/sentiment-analysis-on-moroccan-arabic-dialect/train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65c979e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:28:14.567867Z",
     "iopub.status.busy": "2022-01-21T22:28:14.567282Z",
     "iopub.status.idle": "2022-01-21T22:28:21.510972Z",
     "shell.execute_reply": "2022-01-21T22:28:21.510466Z",
     "shell.execute_reply.started": "2022-01-21T22:25:09.521083Z"
    },
    "papermill": {
     "duration": 6.972557,
     "end_time": "2022-01-21T22:28:21.511114",
     "exception": false,
     "start_time": "2022-01-21T22:28:14.538557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.metrics import confusion_matrix ,recall_score\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79b2ab37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:28:21.572119Z",
     "iopub.status.busy": "2022-01-21T22:28:21.571473Z",
     "iopub.status.idle": "2022-01-21T22:28:21.639024Z",
     "shell.execute_reply": "2022-01-21T22:28:21.638435Z",
     "shell.execute_reply.started": "2022-01-21T22:25:09.529492Z"
    },
    "papermill": {
     "duration": 0.0987,
     "end_time": "2022-01-21T22:28:21.639171",
     "exception": false,
     "start_time": "2022-01-21T22:28:21.540471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/sentiment-analysis-on-moroccan-arabic-dialect/train.csv\")\n",
    "#df.columns = ['comment','label']\n",
    "test = pd.read_csv(\"../input/sentiment-analysis-on-moroccan-arabic-dialect/test_stage1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8e386c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:28:21.716139Z",
     "iopub.status.busy": "2022-01-21T22:28:21.715374Z",
     "iopub.status.idle": "2022-01-21T22:28:21.718450Z",
     "shell.execute_reply": "2022-01-21T22:28:21.719155Z",
     "shell.execute_reply.started": "2022-01-21T22:25:09.571821Z"
    },
    "papermill": {
     "duration": 0.053624,
     "end_time": "2022-01-21T22:28:21.719370",
     "exception": false,
     "start_time": "2022-01-21T22:28:21.665746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 240 entries, 0 to 239\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   ID       240 non-null    int64 \n",
      " 1   comment  240 non-null    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.9+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b2f121d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:28:21.779436Z",
     "iopub.status.busy": "2022-01-21T22:28:21.778852Z",
     "iopub.status.idle": "2022-01-21T22:28:22.565502Z",
     "shell.execute_reply": "2022-01-21T22:28:22.565990Z",
     "shell.execute_reply.started": "2022-01-21T22:25:09.586666Z"
    },
    "papermill": {
     "duration": 0.818061,
     "end_time": "2022-01-21T22:28:22.566170",
     "exception": false,
     "start_time": "2022-01-21T22:28:21.748109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43bc5f68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:28:22.623760Z",
     "iopub.status.busy": "2022-01-21T22:28:22.623203Z",
     "iopub.status.idle": "2022-01-21T22:28:22.629780Z",
     "shell.execute_reply": "2022-01-21T22:28:22.630242Z",
     "shell.execute_reply.started": "2022-01-21T22:25:09.601083Z"
    },
    "papermill": {
     "duration": 0.036995,
     "end_time": "2022-01-21T22:28:22.630404",
     "exception": false,
     "start_time": "2022-01-21T22:28:22.593409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = stopwords.words('arabic')\n",
    "import re\n",
    "#\n",
    "import string\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "#Stemming:\n",
    "#Arabic Light Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a7b2af",
   "metadata": {
    "papermill": {
     "duration": 0.026642,
     "end_time": "2022-01-21T22:28:22.684155",
     "exception": false,
     "start_time": "2022-01-21T22:28:22.657513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2 biblioth√©ques pour stemming utiliser l'une des deux \n",
    "## pour TF IDF utiliser ISRIStemmer \n",
    "## pour word2vec et  FastText utiliser Farasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3449619",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:28:22.743544Z",
     "iopub.status.busy": "2022-01-21T22:28:22.742937Z",
     "iopub.status.idle": "2022-01-21T22:28:22.744795Z",
     "shell.execute_reply": "2022-01-21T22:28:22.745242Z",
     "shell.execute_reply.started": "2022-01-21T22:25:09.610277Z"
    },
    "papermill": {
     "duration": 0.034111,
     "end_time": "2022-01-21T22:28:22.745402",
     "exception": false,
     "start_time": "2022-01-21T22:28:22.711291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem.isri import ISRIStemmer\n",
    "Stemmer=ISRIStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f69ed99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:28:22.802821Z",
     "iopub.status.busy": "2022-01-21T22:28:22.802252Z",
     "iopub.status.idle": "2022-01-21T22:28:33.168029Z",
     "shell.execute_reply": "2022-01-21T22:28:33.167317Z",
     "shell.execute_reply.started": "2022-01-21T22:25:09.622666Z"
    },
    "papermill": {
     "duration": 10.395399,
     "end_time": "2022-01-21T22:28:33.168179",
     "exception": false,
     "start_time": "2022-01-21T22:28:22.772780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting farasapy\r\n",
      "  Downloading farasapy-0.0.14-py3-none-any.whl (11 kB)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from farasapy) (4.62.3)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from farasapy) (2.26.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->farasapy) (3.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->farasapy) (2021.10.8)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->farasapy) (1.26.7)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->farasapy) (2.0.8)\r\n",
      "Installing collected packages: farasapy\r\n",
      "Successfully installed farasapy-0.0.14\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install farasapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4587b486",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:28:33.232370Z",
     "iopub.status.busy": "2022-01-21T22:28:33.231751Z",
     "iopub.status.idle": "2022-01-21T22:28:54.924465Z",
     "shell.execute_reply": "2022-01-21T22:28:54.923841Z",
     "shell.execute_reply.started": "2022-01-21T22:25:18.292763Z"
    },
    "papermill": {
     "duration": 21.727134,
     "end_time": "2022-01-21T22:28:54.924608",
     "exception": false,
     "start_time": "2022-01-21T22:28:33.197474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py:1020: InsecureRequestWarning: Unverified HTTPS request is being made to host 'farasa-api.qcri.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 241M/241M [00:15<00:00, 15.2MiB/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-01-21 22:28:51,194 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    }
   ],
   "source": [
    "from farasa.stemmer import FarasaStemmer\n",
    "Stemmer = FarasaStemmer('arabic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cba287be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:28:55.028157Z",
     "iopub.status.busy": "2022-01-21T22:28:55.027198Z",
     "iopub.status.idle": "2022-01-21T22:28:55.029839Z",
     "shell.execute_reply": "2022-01-21T22:28:55.029350Z",
     "shell.execute_reply.started": "2022-01-21T22:25:22.548165Z"
    },
    "papermill": {
     "duration": 0.059088,
     "end_time": "2022-01-21T22:28:55.029976",
     "exception": false,
     "start_time": "2022-01-21T22:28:54.970888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def removeStopWords(text):\n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]  \n",
    "    text = ' '.join([i for i in filtered_sentence])\n",
    "    return text\n",
    "\n",
    "def NormalizeArabic(text):\n",
    "    text = re.sub(\"[ÿ•ÿ£Ÿ±ÿ¢ÿß]\", \"ÿß\", text)\n",
    "    text = re.sub(\"Ÿâ\", \"Ÿä\", text)\n",
    "    text = re.sub(\"ÿ§\", \"ÿ°\", text)\n",
    "    text = re.sub(\"ÿ¶\", \"ÿ°\", text)\n",
    "    text = re.sub(\"ÿ©\", \"Ÿá\", text)\n",
    "    return text\n",
    "\n",
    "def arabic_diacritics(text):\n",
    "    arabic_diacritics = re.compile(\"\"\" Ÿë    | # Tashdid\n",
    "                             Ÿé    | # Fatha\n",
    "                             Ÿã    | # Tanwin Fath\n",
    "                             Ÿè    | # Damma\n",
    "                             Ÿå    | # Tanwin Damm\n",
    "                             Ÿê    | # Kasra\n",
    "                             Ÿç    | # Tanwin Kasr\n",
    "                             Ÿí    | # Sukun\n",
    "                             ŸÄ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    return text\n",
    "\n",
    "def removeNumbers(text):\n",
    "    \"\"\" Removes integers \"\"\"\n",
    "    text = ''.join([i for i in text if not i.isdigit()])         \n",
    "    return text\n",
    "\n",
    "def stemming(text):\n",
    "    stemmed_words = []\n",
    "    word_tokens = word_tokenize(text) \n",
    "    for w in word_tokens:\n",
    "        stemmed_words.append(Stemmer.stem(w))\n",
    "    stemmed_words = \" \".join(stemmed_words)\n",
    "    return stemmed_words\n",
    "\n",
    "def remove_english_characters(text):\n",
    "        return re.sub(r'[a-zA-Z]+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f2579a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:28:55.128491Z",
     "iopub.status.busy": "2022-01-21T22:28:55.127910Z",
     "iopub.status.idle": "2022-01-21T22:29:12.989472Z",
     "shell.execute_reply": "2022-01-21T22:29:12.988598Z",
     "shell.execute_reply.started": "2022-01-21T22:25:22.563616Z"
    },
    "papermill": {
     "duration": 17.913365,
     "end_time": "2022-01-21T22:29:12.989611",
     "exception": false,
     "start_time": "2022-01-21T22:28:55.076246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    row['comment'] = NormalizeArabic(row['comment'])\n",
    "    row['comment'] = removeStopWords(row['comment'])\n",
    "    row['comment'] = arabic_diacritics(row['comment'])\n",
    "    row['comment'] = removeNumbers(row['comment'])\n",
    "    row['comment'] = row['comment'].translate(translator)\n",
    "    row['comment'] = stemming(row['comment'])\n",
    "    new_df = pd.DataFrame({'comment': [row['comment']]}, index=[index])\n",
    "    df.update(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d5c1f47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:29:13.089083Z",
     "iopub.status.busy": "2022-01-21T22:29:13.087987Z",
     "iopub.status.idle": "2022-01-21T22:29:14.800907Z",
     "shell.execute_reply": "2022-01-21T22:29:14.800379Z",
     "shell.execute_reply.started": "2022-01-21T22:25:42.516422Z"
    },
    "papermill": {
     "duration": 1.765368,
     "end_time": "2022-01-21T22:29:14.801066",
     "exception": false,
     "start_time": "2022-01-21T22:29:13.035698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Arabic Pre-Pre-Processing\n",
    "for index, row in test.iterrows():\n",
    "    row['comment'] = NormalizeArabic(row['comment'])\n",
    "    row['comment'] = removeStopWords(row['comment'])\n",
    "    row['comment'] = arabic_diacritics(row['comment'])\n",
    "    row['comment'] = removeNumbers(row['comment'])\n",
    "    row['comment'] = row['comment'].translate(translator)\n",
    "    row['comment'] = stemming(row['comment'])\n",
    "    new_df = pd.DataFrame({'comment': [row['comment']]}, index=[index])\n",
    "    test.update(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89878779",
   "metadata": {
    "papermill": {
     "duration": 0.045714,
     "end_time": "2022-01-21T22:29:14.892878",
     "exception": false,
     "start_time": "2022-01-21T22:29:14.847164",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **** TF IDF ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09ad8819",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:29:14.990366Z",
     "iopub.status.busy": "2022-01-21T22:29:14.989711Z",
     "iopub.status.idle": "2022-01-21T22:29:15.204637Z",
     "shell.execute_reply": "2022-01-21T22:29:15.204124Z",
     "shell.execute_reply.started": "2022-01-21T22:25:44.281328Z"
    },
    "papermill": {
     "duration": 0.266652,
     "end_time": "2022-01-21T22:29:15.204809",
     "exception": false,
     "start_time": "2022-01-21T22:29:14.938157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ÿ¢ŸÖŸäŸÜ', 'ÿ£ÿ®', 'ÿ£ÿÆ', 'ÿ£ŸÅÿπŸÑ', 'ÿ£ŸÅÿπŸÑŸá', 'ÿ§ŸÑÿßÿ°', 'ÿ•ŸÑ', 'ÿ•ŸÖ', 'ÿßÿ™', 'ÿßÿ™ÿßŸÜ', 'ÿßÿ±ÿ™ÿØ', 'ÿßŸÜ', 'ÿßŸÜŸÅŸÉ', 'ÿ®ÿ±ÿ≠', 'ÿ™ÿßŸÜ', 'ÿ™ÿ®ÿØ', 'ÿ™ÿ≠Ÿà', 'ÿ™ÿπŸÑ', 'ÿ≠ÿØ', 'ÿ≠ŸÖ', 'ÿ≠Ÿä', 'ÿÆÿ®', 'ÿ∞ÿßÿ±', 'ÿ≥ŸäŸÖÿß', 'ÿµŸá', 'ÿ∏ŸÑ', 'ÿ∏ŸÜ', 'ÿπÿØ', 'ŸÇÿ∑', 'ŸÖÿ±', 'ŸÖŸÉÿßŸÜ', 'ŸÖŸÉÿßŸÜŸÉŸÜ', 'ŸÜÿ®', 'Ÿáÿßÿ™', 'Ÿáÿ®', 'ŸàÿßŸáÿß', 'Ÿàÿ±ÿßÿ°'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1920, 9236)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=1, encoding='latin-1', ngram_range=(0,1), stop_words= stop_words)\n",
    "features = tfidf.fit_transform(df['comment']).toarray()\n",
    "labels = df['label']\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4331ac99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:29:15.310739Z",
     "iopub.status.busy": "2022-01-21T22:29:15.307423Z",
     "iopub.status.idle": "2022-01-21T22:29:15.327614Z",
     "shell.execute_reply": "2022-01-21T22:29:15.328176Z",
     "shell.execute_reply.started": "2022-01-21T22:25:44.528947Z"
    },
    "papermill": {
     "duration": 0.076404,
     "end_time": "2022-01-21T22:29:15.328347",
     "exception": false,
     "start_time": "2022-01-21T22:29:15.251943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 9236)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF Features Extraction\n",
    "features_unseen = tfidf.transform(test['comment']).toarray()\n",
    "features_unseen.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4f2b61",
   "metadata": {
    "papermill": {
     "duration": 0.045911,
     "end_time": "2022-01-21T22:29:15.420745",
     "exception": false,
     "start_time": "2022-01-21T22:29:15.374834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3eef49e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:29:15.517840Z",
     "iopub.status.busy": "2022-01-21T22:29:15.517178Z",
     "iopub.status.idle": "2022-01-21T22:29:16.232469Z",
     "shell.execute_reply": "2022-01-21T22:29:16.233864Z",
     "shell.execute_reply.started": "2022-01-21T22:25:44.561653Z"
    },
    "papermill": {
     "duration": 0.766209,
     "end_time": "2022-01-21T22:29:16.234280",
     "exception": false,
     "start_time": "2022-01-21T22:29:15.468071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76041667 0.78645833 0.734375   0.80208333 0.76822917]\n",
      "0.7527348748128146\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.80      0.77       147\n",
      "           1       0.77      0.70      0.74       141\n",
      "\n",
      "    accuracy                           0.75       288\n",
      "   macro avg       0.76      0.75      0.75       288\n",
      "weighted avg       0.76      0.75      0.75       288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix ,recall_score\n",
    "from sklearn.metrics import f1_score,classification_report\n",
    "def cnb(c):\n",
    "    X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.15, random_state=14)#32 #14\n",
    "    model_cnb = ComplementNB(alpha=1,norm=False)\n",
    "    scores = cross_val_score(model_cnb, features, labels, cv=5)\n",
    "    print(scores)\n",
    "    model_cnb.fit(X_train, y_train)\n",
    "    predict=model_cnb.predict(X_test)\n",
    "    print(f1_score(y_test,predict, average='weighted'))\n",
    "    print(metrics.classification_report(y_test,predict))\n",
    "for i in range(1):\n",
    "    cnb(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a068659",
   "metadata": {
    "papermill": {
     "duration": 0.051534,
     "end_time": "2022-01-21T22:29:16.379492",
     "exception": false,
     "start_time": "2022-01-21T22:29:16.327958",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **LogisticRegression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3172076",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:29:16.484368Z",
     "iopub.status.busy": "2022-01-21T22:29:16.482883Z",
     "iopub.status.idle": "2022-01-21T22:29:16.929165Z",
     "shell.execute_reply": "2022-01-21T22:29:16.930609Z",
     "shell.execute_reply.started": "2022-01-21T22:25:45.304499Z"
    },
    "papermill": {
     "duration": 0.504943,
     "end_time": "2022-01-21T22:29:16.930911",
     "exception": false,
     "start_time": "2022-01-21T22:29:16.425968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7705627705627707\n",
      "0.7705627705627706\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.79      0.77       113\n",
      "           1       0.79      0.75      0.77       118\n",
      "\n",
      "    accuracy                           0.77       231\n",
      "   macro avg       0.77      0.77      0.77       231\n",
      "weighted avg       0.77      0.77      0.77       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix ,recall_score\n",
    "def reg(c):\n",
    "    X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.12, random_state=3)#39#32 #14\n",
    "    logreg = LogisticRegression(C=4,fit_intercept=True)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    predict= logreg.predict(X_test)\n",
    "    y_pred_unseen= logreg.predict(features_unseen)\n",
    "    from sklearn.metrics import f1_score\n",
    "    print(f1_score(predict, y_test, average='weighted'))\n",
    "    print(recall_score(predict,y_test,average='micro'))\n",
    "    print(metrics.classification_report(predict,y_test))\n",
    "    submission = pd.read_csv(\"../input/sentiment-analysis-on-moroccan-arabic-dialect/sample_submission.csv\")\n",
    "    submission['label'] = y_pred_unseen\n",
    "    submission = submission[['ID','label']]\n",
    "    #submission.to_csv('tfidf.csv', index=False)\n",
    "    \n",
    "for i in range(1):\n",
    "    reg(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf6cfe",
   "metadata": {
    "papermill": {
     "duration": 0.047758,
     "end_time": "2022-01-21T22:29:17.067149",
     "exception": false,
     "start_time": "2022-01-21T22:29:17.019391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SGDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af4a8132",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:29:17.169088Z",
     "iopub.status.busy": "2022-01-21T22:29:17.168457Z",
     "iopub.status.idle": "2022-01-21T22:29:18.544370Z",
     "shell.execute_reply": "2022-01-21T22:29:18.545501Z",
     "shell.execute_reply.started": "2022-01-21T22:25:45.791092Z"
    },
    "papermill": {
     "duration": 1.431513,
     "end_time": "2022-01-21T22:29:18.545961",
     "exception": false,
     "start_time": "2022-01-21T22:29:17.114448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7742647275587555\n",
      "0.7743055555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix ,recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "# Always scale the input. The most convenient way is to use a pipeline.\n",
    "\n",
    "def svm(c):\n",
    "    arr=[]\n",
    "    X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.15, random_state=3)#32 #14\n",
    "    model_sgd = SGDClassifier(max_iter=1000, tol=1e-4,loss='log',random_state=45,alpha=0.0001,penalty='l2')\n",
    "    #model_sgd = make_pipeline(StandardScaler(),SGDClassifier(max_iter=3000, loss='log',tol=1e-4,random_state =c))\n",
    "    #model_sgd = linear_model.SGDClassifier(max_iter=1000,eta0=0.01, alpha=0.0001, loss='log', random_state=2, penalty='l2', tol=1e-4, learning_rate='constant')\n",
    "    #clf.fit(X_train, Y)\n",
    "    model_sgd.fit(X_train, y_train)\n",
    "    predict=model_sgd.predict(X_test)\n",
    "    y_pred_unseen = model_sgd.predict(features_unseen)\n",
    "    print(f1_score(predict,y_test, average='weighted'))\n",
    "    print(recall_score(predict,y_test,average='micro'))\n",
    "    submission = pd.read_csv(\"../input/sentiment-analysis-on-moroccan-arabic-dialect/sample_submission.csv\")\n",
    "    submission['label'] = y_pred_unseen\n",
    "    submission = submission[['ID','label']]\n",
    "    #submission.to_csv('tfidf(SGDC).csv', index=False)  \n",
    "for i in range(1):\n",
    "    svm(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f87f85e",
   "metadata": {
    "papermill": {
     "duration": 0.04679,
     "end_time": "2022-01-21T22:29:18.694794",
     "exception": false,
     "start_time": "2022-01-21T22:29:18.648004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42f18162",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:29:18.797584Z",
     "iopub.status.busy": "2022-01-21T22:29:18.796223Z",
     "iopub.status.idle": "2022-01-21T22:30:04.126388Z",
     "shell.execute_reply": "2022-01-21T22:30:04.126849Z",
     "shell.execute_reply.started": "2022-01-21T22:25:47.186379Z"
    },
    "papermill": {
     "duration": 45.385377,
     "end_time": "2022-01-21T22:30:04.127007",
     "exception": false,
     "start_time": "2022-01-21T22:29:18.741630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568975225225225\n",
      "0.7569444444444444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix ,recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "def svm(c):\n",
    "    arr=[]\n",
    "    X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.15, random_state=14)#32 #14\n",
    "    model_svc = SVC(kernel='poly',degree=1)\n",
    "    model_svc.fit(X_train, y_train)\n",
    "    y_pred_svc = model_svc.predict(X_test)\n",
    "    y_pred_unseen=model_svc.predict(features_unseen)\n",
    "    print(f1_score(y_test, y_pred_svc, average='weighted'))\n",
    "    print(recall_score(y_test,y_pred_svc ,average='micro'))\n",
    "    submission = pd.read_csv(\"../input/sentiment-analysis-on-moroccan-arabic-dialect/sample_submission.csv\")\n",
    "    submission['label'] = y_pred_unseen\n",
    "    submission = submission[['ID','label']]\n",
    "    #submission.to_csv('tfidf(svm).csv', index=False)  \n",
    "for i in range(1):\n",
    "    svm(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a371ea",
   "metadata": {
    "papermill": {
     "duration": 0.047352,
     "end_time": "2022-01-21T22:30:04.223137",
     "exception": false,
     "start_time": "2022-01-21T22:30:04.175785",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ****Word2vec****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d543694",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:30:04.326268Z",
     "iopub.status.busy": "2022-01-21T22:30:04.325395Z",
     "iopub.status.idle": "2022-01-21T22:30:04.328592Z",
     "shell.execute_reply": "2022-01-21T22:30:04.329032Z",
     "shell.execute_reply.started": "2022-01-21T22:26:32.261397Z"
    },
    "papermill": {
     "duration": 0.058741,
     "end_time": "2022-01-21T22:30:04.329201",
     "exception": false,
     "start_time": "2022-01-21T22:30:04.270460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1920, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi = df.append(test, ignore_index=True)\n",
    "df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97d204cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:30:04.430217Z",
     "iopub.status.busy": "2022-01-21T22:30:04.429494Z",
     "iopub.status.idle": "2022-01-21T22:30:15.172990Z",
     "shell.execute_reply": "2022-01-21T22:30:15.173416Z",
     "shell.execute_reply.started": "2022-01-21T22:26:32.269942Z"
    },
    "papermill": {
     "duration": 10.796797,
     "end_time": "2022-01-21T22:30:15.173597",
     "exception": false,
     "start_time": "2022-01-21T22:30:04.376800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2253607, 3323150)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "tokenized_tweet = combi['comment'].apply(lambda x: str(x).split()) # tokenizing\n",
    "\n",
    "model_w2v = Word2Vec(tokenized_tweet, vector_size=300, window=20,min_count=5, workers=2, epochs=50)\n",
    "\n",
    "model_w2v.train(tokenized_tweet, total_examples= len(combi['comment']), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "311d0799",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:30:15.277039Z",
     "iopub.status.busy": "2022-01-21T22:30:15.276060Z",
     "iopub.status.idle": "2022-01-21T22:30:15.278155Z",
     "shell.execute_reply": "2022-01-21T22:30:15.278746Z",
     "shell.execute_reply.started": "2022-01-21T22:26:44.143067Z"
    },
    "papermill": {
     "duration": 0.056569,
     "end_time": "2022-01-21T22:30:15.278919",
     "exception": false,
     "start_time": "2022-01-21T22:30:15.222350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = list(model_w2v.wv.index_to_key)\n",
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v.wv[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not in vocabulary\n",
    "                         \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8d7707c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:30:15.383619Z",
     "iopub.status.busy": "2022-01-21T22:30:15.382601Z",
     "iopub.status.idle": "2022-01-21T22:30:15.628589Z",
     "shell.execute_reply": "2022-01-21T22:30:15.629003Z",
     "shell.execute_reply.started": "2022-01-21T22:26:44.150929Z"
    },
    "papermill": {
     "duration": 0.302206,
     "end_time": "2022-01-21T22:30:15.629178",
     "exception": false,
     "start_time": "2022-01-21T22:30:15.326972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2160, 300)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 300))\n",
    "\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 300)\n",
    "    \n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "wordvec_df.shape\n",
    "#tokenized_tweet[252]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d677749b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:30:15.732967Z",
     "iopub.status.busy": "2022-01-21T22:30:15.731712Z",
     "iopub.status.idle": "2022-01-21T22:30:15.740724Z",
     "shell.execute_reply": "2022-01-21T22:30:15.741187Z",
     "shell.execute_reply.started": "2022-01-21T22:26:44.542609Z"
    },
    "papermill": {
     "duration": 0.063925,
     "end_time": "2022-01-21T22:30:15.741351",
     "exception": false,
     "start_time": "2022-01-21T22:30:15.677426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2160, 300) (1920,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_w2v = wordvec_df.iloc[:1920,:]\n",
    "test_w2v = wordvec_df.iloc[1920:,:]\n",
    "\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train_w2v, df['label'],test_size=0.15, random_state=12)\n",
    "\n",
    "print(wordvec_df.shape, df['label'].shape)\n",
    "#len(ytrain.index)#1457\n",
    "len(yvalid.index)#365\n",
    "#len(train_w2v)\n",
    "xtrain_w2v = train_w2v.iloc[ytrain.index,:]\n",
    "xvalid_w2v = train_w2v.iloc[yvalid.index,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44236ede",
   "metadata": {
    "papermill": {
     "duration": 0.04812,
     "end_time": "2022-01-21T22:30:15.838105",
     "exception": false,
     "start_time": "2022-01-21T22:30:15.789985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15e04eaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:30:15.949892Z",
     "iopub.status.busy": "2022-01-21T22:30:15.942493Z",
     "iopub.status.idle": "2022-01-21T22:30:17.188423Z",
     "shell.execute_reply": "2022-01-21T22:30:17.189174Z",
     "shell.execute_reply.started": "2022-01-21T22:26:44.557601Z"
    },
    "papermill": {
     "duration": 1.302971,
     "end_time": "2022-01-21T22:30:17.189412",
     "exception": false,
     "start_time": "2022-01-21T22:30:15.886441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8159722222222222\n",
      "0.8159921907548289\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix ,recall_score\n",
    "def  svc(c):\n",
    "    train_w2v = wordvec_df.iloc[:1920,:]\n",
    "    test_w2v = wordvec_df.iloc[1920:,:]\n",
    "\n",
    "    xtrain, xvalid, ytrain, yvalid = train_test_split(train_w2v, df['label'],test_size=0.15, random_state=4)\n",
    "    clf = svm.SVC(kernel='rbf',probability=False,C=1)\n",
    "    clf.fit(xtrain,ytrain)\n",
    "    predict = clf.predict(xvalid)\n",
    "    submission2 = pd.read_csv(\"../input/sentiment-analysis-on-moroccan-arabic-dialect/sample_submission.csv\")\n",
    "    test_pred = clf.predict(test_w2v)\n",
    "    print(recall_score(yvalid,predict,average='micro'))\n",
    "    print(f1_score(yvalid,predict, average='weighted'))\n",
    "for i in range(1):\n",
    "    svc(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341b3172",
   "metadata": {
    "papermill": {
     "duration": 0.051173,
     "end_time": "2022-01-21T22:30:17.309779",
     "exception": false,
     "start_time": "2022-01-21T22:30:17.258606",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SGDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da902fc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:30:17.410248Z",
     "iopub.status.busy": "2022-01-21T22:30:17.409548Z",
     "iopub.status.idle": "2022-01-21T22:30:17.521203Z",
     "shell.execute_reply": "2022-01-21T22:30:17.522173Z",
     "shell.execute_reply.started": "2022-01-21T22:26:45.803613Z"
    },
    "papermill": {
     "duration": 0.1643,
     "end_time": "2022-01-21T22:30:17.522444",
     "exception": false,
     "start_time": "2022-01-21T22:30:17.358144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6979166666666666\n",
      "0.6979166666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix ,recall_score\n",
    "# Always scale the input. The most convenient way is to use a pipeline.\n",
    "\n",
    "def svm(c):\n",
    "    train_w2v = wordvec_df.iloc[:1920,:]\n",
    "    test_w2v = wordvec_df.iloc[1920:,:]\n",
    "\n",
    "    xtrain, xvalid, ytrain, yvalid = train_test_split(train_w2v, df['label'],test_size=0.1, random_state=39)  \n",
    "    #model_sgd = SGDClassifier(max_iter=3000, tol=1e-4,loss='huber',random_state =4)\n",
    "    #model_sgd = make_pipeline(StandardScaler(),SGDClassifier(max_iter=3000, tol=1e-4,random_state=0))\n",
    "    model_sgd = linear_model.SGDClassifier(max_iter=3000,eta0=0.01, alpha=0.0001, loss='log', random_state=0, penalty='l2', tol=1e-4, learning_rate='constant')\n",
    "    #clf.fit(X_train, Y)\n",
    "    model_sgd.fit(xtrain, ytrain)\n",
    "    y_pred_unseen = model_sgd.predict(test_w2v)\n",
    "    predict=model_sgd.predict(xvalid)\n",
    "    print(recall_score(yvalid,predict,average='micro'))\n",
    "    print(f1_score(yvalid, predict, average='weighted'))\n",
    "    #print('num√©ro:',c,metrics.classification_report(y,y_pred_unseen))    \n",
    "for i in range(1):\n",
    "    svm(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c39103b",
   "metadata": {
    "papermill": {
     "duration": 0.055769,
     "end_time": "2022-01-21T22:30:17.675596",
     "exception": false,
     "start_time": "2022-01-21T22:30:17.619827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Logistique Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2082e204",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:30:17.781700Z",
     "iopub.status.busy": "2022-01-21T22:30:17.780968Z",
     "iopub.status.idle": "2022-01-21T22:30:17.996107Z",
     "shell.execute_reply": "2022-01-21T22:30:17.996927Z",
     "shell.execute_reply.started": "2022-01-21T22:26:45.925402Z"
    },
    "papermill": {
     "duration": 0.272696,
     "end_time": "2022-01-21T22:30:17.997190",
     "exception": false,
     "start_time": "2022-01-21T22:30:17.724494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7291666666666666\n",
      "0.7286435951909613\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix ,recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "def reg(c):\n",
    "    train_w2v = wordvec_df.iloc[:1920,:]\n",
    "    test_w2v = wordvec_df.iloc[1920:,:]\n",
    "    xtrain, xvalid, ytrain, yvalid = train_test_split(train_w2v, df['label'],test_size=0.15, random_state=2) \n",
    "    logreg = LogisticRegression(C=2,fit_intercept=True,solver='liblinear')\n",
    "    logreg.fit(xtrain, ytrain)\n",
    "    predict=logreg.predict(xvalid)\n",
    "    y_pred_unseen= logreg.predict(test_w2v)\n",
    "    print(recall_score(yvalid,predict,average='micro'))\n",
    "    print(f1_score(yvalid,predict, average='weighted'))\n",
    "for i in range(1):\n",
    "    reg(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e2e3ad",
   "metadata": {
    "papermill": {
     "duration": 0.056194,
     "end_time": "2022-01-21T22:30:18.154079",
     "exception": false,
     "start_time": "2022-01-21T22:30:18.097885",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ****FastText****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8551bd07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:30:18.259877Z",
     "iopub.status.busy": "2022-01-21T22:30:18.258918Z",
     "iopub.status.idle": "2022-01-21T22:30:58.990818Z",
     "shell.execute_reply": "2022-01-21T22:30:58.991301Z",
     "shell.execute_reply.started": "2022-01-21T22:26:46.198958Z"
    },
    "papermill": {
     "duration": 40.787847,
     "end_time": "2022-01-21T22:30:58.991471",
     "exception": false,
     "start_time": "2022-01-21T22:30:18.203624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a Gensim FastText model\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "combi = df.append(test, ignore_index=True)\n",
    "tokenized_tweet = combi['comment'].apply(lambda x: str(x).split())\n",
    "print(\"Training a Gensim FastText model\")\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "\n",
    "model = FastText(tokenized_tweet,sg=4,vector_size=300,window=20,epochs=40)#241#300\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1893817c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:30:59.093308Z",
     "iopub.status.busy": "2022-01-21T22:30:59.092442Z",
     "iopub.status.idle": "2022-01-21T22:30:59.097543Z",
     "shell.execute_reply": "2022-01-21T22:30:59.098042Z",
     "shell.execute_reply.started": "2022-01-21T22:27:30.106068Z"
    },
    "papermill": {
     "duration": 0.05769,
     "end_time": "2022-01-21T22:30:59.098197",
     "exception": false,
     "start_time": "2022-01-21T22:30:59.040507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model.wv[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not in vocabulary\n",
    "                         \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91edf1a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:30:59.202044Z",
     "iopub.status.busy": "2022-01-21T22:30:59.201146Z",
     "iopub.status.idle": "2022-01-21T22:30:59.882401Z",
     "shell.execute_reply": "2022-01-21T22:30:59.882827Z",
     "shell.execute_reply.started": "2022-01-21T22:27:30.115382Z"
    },
    "papermill": {
     "duration": 0.73536,
     "end_time": "2022-01-21T22:30:59.883004",
     "exception": false,
     "start_time": "2022-01-21T22:30:59.147644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2160, 300)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_arrays = np.zeros((len(tokenized_tweet),300))\n",
    "\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    fasttext_arrays[i,:] = word_vector(tokenized_tweet[i], 300)\n",
    "    #fasttext_arrays[i]=word_vector(tokenized_tweet[i],300)\n",
    "fasttext_df = pd.DataFrame(fasttext_arrays)\n",
    "fasttext_df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b301a04e",
   "metadata": {
    "papermill": {
     "duration": 0.049625,
     "end_time": "2022-01-21T22:30:59.983033",
     "exception": false,
     "start_time": "2022-01-21T22:30:59.933408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## apr√©s l'√©x√©cution de fast text nous avons extrait le vecteur suivant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "160c108e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:31:00.087146Z",
     "iopub.status.busy": "2022-01-21T22:31:00.086187Z",
     "iopub.status.idle": "2022-01-21T22:31:00.089588Z",
     "shell.execute_reply": "2022-01-21T22:31:00.090196Z",
     "shell.execute_reply.started": "2022-01-21T22:27:31.121591Z"
    },
    "papermill": {
     "duration": 0.057493,
     "end_time": "2022-01-21T22:31:00.090355",
     "exception": false,
     "start_time": "2022-01-21T22:31:00.032862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fasttext_df=pd.read_csv('../input/competition/vecteur(SGDC).csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f86b689",
   "metadata": {
    "papermill": {
     "duration": 0.05237,
     "end_time": "2022-01-21T22:31:00.193909",
     "exception": false,
     "start_time": "2022-01-21T22:31:00.141539",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SGDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef1a5846",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:31:00.301380Z",
     "iopub.status.busy": "2022-01-21T22:31:00.300768Z",
     "iopub.status.idle": "2022-01-21T22:31:00.858860Z",
     "shell.execute_reply": "2022-01-21T22:31:00.859636Z",
     "shell.execute_reply.started": "2022-01-21T22:27:31.127538Z"
    },
    "papermill": {
     "duration": 0.616468,
     "end_time": "2022-01-21T22:31:00.859919",
     "exception": false,
     "start_time": "2022-01-21T22:31:00.243451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7395833333333334\n",
      "fscore 0.7386727965640313\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import linear_model\n",
    "def sgd(c):\n",
    "    train_ft = fasttext_df.iloc[:1920,:]\n",
    "    test_ft = fasttext_df.iloc[1920:,:]\n",
    "    xtrain, xvalid, ytrain, yvalid = train_test_split(train_ft, df['label'], test_size=0.15,random_state=99)#99\n",
    "    #model_sgd = SGDClassifier(max_iter=3000, tol=1e-4,loss='log',random_state =4)\n",
    "    #model_sgd = make_pipeline(SGDClassifier(max_iter=3000,eta0=0.01,alpha=0.0001,loss='log', penalty='elasticnet',tol=1e-4,random_state=21,learning_rate='constant'))#21\n",
    "    model_sgd = linear_model.SGDClassifier(max_iter=1000,eta0=0.01, alpha=0.00001, loss='log', random_state=1, penalty='elasticnet', tol=1e-4,learning_rate='constant')\n",
    "    #clf.fit(X_train, Y) eta0=0.01, alpha=0.0001\n",
    "    model_sgd.fit(xtrain, ytrain)\n",
    "    y_pred_unseen = model_sgd.predict(test_ft)\n",
    "    predict=model_sgd.predict(xvalid)\n",
    "    print(recall_score(yvalid,predict,average='micro'))\n",
    "    print('fscore',f1_score(yvalid, predict, average='weighted'))\n",
    "    #print('num√©ro:',c,metrics.classification_report(y,y_pred_unseen))   \n",
    "    submission = pd.read_csv(\"../input/sentiment-analysis-on-moroccan-arabic-dialect/sample_submission.csv\")\n",
    "    submission['label'] = y_pred_unseen\n",
    "    submission = submission[['ID','label']]\n",
    "    #submission.to_csv('SGDC.csv', index=False)\n",
    "for i in range(1):\n",
    "    sgd(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574dd08f",
   "metadata": {
    "papermill": {
     "duration": 0.05157,
     "end_time": "2022-01-21T22:31:01.011352",
     "exception": false,
     "start_time": "2022-01-21T22:31:00.959782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Regression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2db27af8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:31:01.118804Z",
     "iopub.status.busy": "2022-01-21T22:31:01.114061Z",
     "iopub.status.idle": "2022-01-21T22:31:01.228737Z",
     "shell.execute_reply": "2022-01-21T22:31:01.229533Z",
     "shell.execute_reply.started": "2022-01-21T22:27:31.639068Z"
    },
    "papermill": {
     "duration": 0.168378,
     "end_time": "2022-01-21T22:31:01.229819",
     "exception": false,
     "start_time": "2022-01-21T22:31:01.061441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.7487885249079278\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def reg(c):\n",
    "    arr=[]\n",
    "    train_ft = fasttext_df.iloc[:1920,:]\n",
    "    test_ft = fasttext_df.iloc[1920:,:]\n",
    "    xtrain, xvalid, ytrain, yvalid = train_test_split(train_ft, df['label'], test_size=0.15,random_state=13)#66 #76\n",
    "    logreg = LogisticRegression(C=1,fit_intercept=True)\n",
    "    logreg.fit(xtrain, ytrain)\n",
    "    y_pred_unseen= logreg.predict(test_ft)\n",
    "    predict= logreg.predict(xvalid)\n",
    "    print(recall_score(yvalid,predict,average='micro'))\n",
    "    print(f1_score(yvalid, predict, average='weighted'))\n",
    "    submission2 = pd.read_csv(\"../input/sentiment-analysis-on-moroccan-arabic-dialect/sample_submission.csv\")\n",
    "    submission2['label'] = y_pred_unseen\n",
    "    submission2 = submission2[['ID','label']]\n",
    "    #submission2.to_csv('regress.csv', index=False)\n",
    "for i in range(1):\n",
    "    reg(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5a1eb",
   "metadata": {
    "papermill": {
     "duration": 0.050662,
     "end_time": "2022-01-21T22:31:01.409180",
     "exception": false,
     "start_time": "2022-01-21T22:31:01.358518",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "807bd8bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T22:31:01.520516Z",
     "iopub.status.busy": "2022-01-21T22:31:01.519149Z",
     "iopub.status.idle": "2022-01-21T22:31:02.765816Z",
     "shell.execute_reply": "2022-01-21T22:31:02.765329Z",
     "shell.execute_reply.started": "2022-01-21T22:27:31.762132Z"
    },
    "papermill": {
     "duration": 1.306124,
     "end_time": "2022-01-21T22:31:02.765967",
     "exception": false,
     "start_time": "2022-01-21T22:31:01.459843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7430555555555556\n",
      "0.7409442310497398\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix ,recall_score\n",
    "def  svc(c):\n",
    "    arr=[]\n",
    "    train_ft = fasttext_df.iloc[:1920,:]\n",
    "    test_ft = fasttext_df.iloc[1920:,:]\n",
    "    xtrain, xvalid, ytrain, yvalid = train_test_split(train_ft, df['label'], test_size=0.15,random_state=13)#12#16\n",
    "    clf = svm.SVC(kernel='sigmoid',probability=False,C=1,gamma='scale')  \n",
    "    clf.fit(xtrain,ytrain)\n",
    "    y_pred_unseen = clf.predict(test_ft)\n",
    "    predict = clf.predict(xvalid)\n",
    "    print(recall_score(yvalid,predict,average='micro'))\n",
    "    print(f1_score(yvalid, predict, average='weighted'))\n",
    "    submission = pd.read_csv(\"../input/sentiment-analysis-on-moroccan-arabic-dialect/sample_submission.csv\")\n",
    "    submission['label'] = y_pred_unseen\n",
    "    submission = submission[['ID','label']]\n",
    "    #submission.to_csv('SVM.csv', index=False)\n",
    "for i in range(1):\n",
    "    svc(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3868c78",
   "metadata": {
    "papermill": {
     "duration": 0.052421,
     "end_time": "2022-01-21T22:31:02.873031",
     "exception": false,
     "start_time": "2022-01-21T22:31:02.820610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57833a7b",
   "metadata": {
    "papermill": {
     "duration": 0.05071,
     "end_time": "2022-01-21T22:31:02.974977",
     "exception": false,
     "start_time": "2022-01-21T22:31:02.924267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 180.706041,
   "end_time": "2022-01-21T22:31:06.513009",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-21T22:28:05.806968",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
